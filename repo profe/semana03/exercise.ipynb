{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: Data Cleaning - Guided Exercise\n",
    "\n",
    "**Duration:** ~30 minutes  \n",
    "**Dataset:** Education Statistics from the Colombian Ministry of Education (datos.gov.co)  \n",
    "**Rows:** 482 (dirty) | **Columns:** 37  \n",
    "**Goal:** Clean this dataset by fixing 5 types of data quality issues  \n",
    "\n",
    "---\n",
    "\n",
    "**How to use this notebook:**\n",
    "\n",
    "1. Read each explanation cell carefully\n",
    "2. Run the code cell below it (Shift + Enter)\n",
    "3. Read the \"What just happened\" follow-up\n",
    "4. Answer the questions or complete the \"Your Turn\" exercises\n",
    "\n",
    "Think of this entire process as doing laundry: we need to sort, wash, dry, and fold our data before it is ready to wear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We start by importing our tools and loading the CSV file. We also make a copy of the original data so we can compare before and after at the end.\n",
    "\n",
    "**Run the cell below** to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('data/educacion_estadisticas.csv')\n",
    "\n",
    "# Keep a copy of the original so we can compare at the end\n",
    "df_original = df.copy()\n",
    "\n",
    "print(f\"Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has 482 rows and 37 columns. Each row represents one Colombian department in one year (2011-2024), with education indicators like enrollment rates, dropout rates, and coverage.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Data Inspection\n",
    "\n",
    "Before cleaning anything, we need to understand what we are working with. Think of this as **opening the laundry bag** and checking what is inside before turning on the washing machine. You would not throw everything in without looking first.\n",
    "\n",
    "This is the **inspection ritual**: a set of commands you run at the start of every data project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Shape and Column Names\n",
    "\n",
    "We use `df.shape` to see how many rows and columns exist, and `df.columns.tolist()` to see the names of all columns.\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Rows: {df.shape[0]}\")\n",
    "print(f\"Columns: {df.shape[1]}\")\n",
    "print()\n",
    "print(\"Column names:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What to notice:** The column names are in Spanish (as expected from a Colombian government dataset). Key columns include:\n",
    "- `ano` = year\n",
    "- `departamento` = department (geographic region)\n",
    "- `poblacion_5_16` = population aged 5-16\n",
    "- `desercion` = dropout rate\n",
    "- `cobertura_neta` = net enrollment coverage\n",
    "- `aprobacion` / `reprobacion` = approval / failure rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 First Rows\n",
    "\n",
    "Looking at the actual data helps us spot problems that column names alone cannot reveal. `df.head()` shows the first 5 rows.\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Data Types\n",
    "\n",
    "Every column has a data type that tells pandas how to store and process the values. We use `df.dtypes` to see them.\n",
    "\n",
    "**Look for suspicious types:** a year stored as `float64` instead of `int64`, or a population number stored as `object` (text).\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened:** pandas tells us the type of each column.\n",
    "\n",
    "**Key findings:**\n",
    "- `ano` is `float64` (decimal) but years should be whole numbers (`int64`). Years like 2023.0 look odd.\n",
    "- `poblacion_5_16` is `object` (text) even though it should be a number. This means some values contain characters that prevent pandas from reading them as numbers.\n",
    "- Most rate columns are `float64`, which is correct for percentages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Missing Values\n",
    "\n",
    "`isnull().sum()` counts how many NaN (missing) values each column has. We sort the result so the worst offenders appear first.\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = df.isnull().sum()\n",
    "missing_only = missing[missing > 0].sort_values(ascending=False)\n",
    "\n",
    "print(f\"Columns with missing values: {len(missing_only)} of {len(df.columns)}\")\n",
    "print(f\"Total missing cells: {missing.sum()} of {df.shape[0] * df.shape[1]:,}\")\n",
    "print()\n",
    "print(missing_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Statistical Summary\n",
    "\n",
    "`df.describe()` gives us count, mean, std, min, max, and percentiles for every numeric column. This helps spot outliers and confirms what we learned from `isnull()`.\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened:** The `count` row shows how many non-null values each column has. Columns where `count` is less than 482 (our total rows) have missing data. Also look at `min` and `max`: do any values look impossible? We will come back to this in Section 6.\n",
    "\n",
    "### Inspection Summary\n",
    "\n",
    "From our inspection, we found **5 data quality issues** to fix:\n",
    "\n",
    "| # | Issue | Where |\n",
    "|---|-------|-------|\n",
    "| 1 | Missing values (NaN) | Multiple columns, especially `sedes_conectadas_a_internet`, `tamano_promedio_grupo` |\n",
    "| 2 | Wrong data types | `ano` is float64, `poblacion_5_16` is object |\n",
    "| 3 | Duplicate rows | 482 rows but only 462 expected (20 extra) |\n",
    "| 4 | Text inconsistencies | `departamento` has too many unique values |\n",
    "| 5 | Invalid values | Some percentages might be negative or above 100 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 1\n",
    "\n",
    "Based on the inspection you just ran, **list 3 specific problems** you noticed. For each one, note:\n",
    "- Which column is affected\n",
    "- What the problem is\n",
    "- How you spotted it (which command revealed it)\n",
    "\n",
    "*Double-click this cell and write your answer below:*\n",
    "\n",
    "1. ...\n",
    "2. ...\n",
    "3. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Missing Values\n",
    "\n",
    "**What is NaN?** NaN stands for \"Not a Number.\" It is pandas' way of saying \"this value is missing.\" Think of it like reaching into your sock drawer and finding... nothing. The sock is not there. It is not zero socks (that would mean you counted and found none). It is \"unknown.\"\n",
    "\n",
    "**Why do missing values appear?** Data can be missing because:\n",
    "- It was never collected (a survey question left blank)\n",
    "- It was lost during processing (a system error)\n",
    "- It does not apply (internet connectivity data before internet existed in schools)\n",
    "\n",
    "Like socks without a pair: you need to decide whether to find a replacement, toss them, or accept the mismatch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Missing Value Percentages\n",
    "\n",
    "Before deciding what to do, we need to know how bad the problem is. Let's calculate the percentage of missing values per column.\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_pct = (df.isnull().sum() / len(df) * 100).round(2)\n",
    "missing_pct = missing_pct[missing_pct > 0].sort_values(ascending=False)\n",
    "\n",
    "print(\"Missing value percentages:\")\n",
    "print(missing_pct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 The Decision Framework\n",
    "\n",
    "Now we apply the decision framework from class:\n",
    "\n",
    "| Missing % | Action | Reasoning |\n",
    "|-----------|--------|----------|\n",
    "| > 50% | Consider dropping the column | More gaps than data |\n",
    "| < 5% | Safe to drop the rows | Losing very few rows |\n",
    "| 5-50% | Fill with an appropriate value | Too many rows to lose, need to estimate |\n",
    "\n",
    "Applying this to our columns:\n",
    "- `sedes_conectadas_a_internet`, `tamano_promedio_grupo`: ~50% missing. Fill with 0 (\"not reported\").\n",
    "- `departamento`: ~2-3% missing. Drop those rows (critical identifier, cannot guess).\n",
    "- Rate columns (`desercion`, `cobertura_neta`, `aprobacion`, `reprobacion`): 8-11% missing. Fill with median.\n",
    "- `poblacion_5_16`: ~10% missing. We will fix this in Section 3 (it also has type problems)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Fill Count-Based Columns with 0\n",
    "\n",
    "For `sedes_conectadas_a_internet` (% schools with internet) and `tamano_promedio_grupo` (average class size), the data only exists through 2017. After that, it was not reported. Filling with 0 means \"no data available for this period.\"\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Before fillna:\")\n",
    "print(f\"  sedes_conectadas_a_internet NaN: {df['sedes_conectadas_a_internet'].isnull().sum()}\")\n",
    "print(f\"  tamano_promedio_grupo NaN:       {df['tamano_promedio_grupo'].isnull().sum()}\")\n",
    "\n",
    "df['sedes_conectadas_a_internet'] = df['sedes_conectadas_a_internet'].fillna(0)\n",
    "df['tamano_promedio_grupo'] = df['tamano_promedio_grupo'].fillna(0)\n",
    "\n",
    "print(f\"\\nAfter fillna:\")\n",
    "print(f\"  sedes_conectadas_a_internet NaN: {df['sedes_conectadas_a_internet'].isnull().sum()}\")\n",
    "print(f\"  tamano_promedio_grupo NaN:       {df['tamano_promedio_grupo'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened:** We replaced all NaN values in those two columns with 0. The missing count went from ~240 to 0 for each.\n",
    "\n",
    "**Why 0 is appropriate here but NOT for rates:** These columns represent counts or measurements that simply were not reported after 2017. Using 0 signals \"not available.\" But filling a dropout rate with 0 would be misleading: 0% dropout means \"nobody dropped out\" (a strong claim), not \"we don't know.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Drop Rows Where `departamento` is Missing\n",
    "\n",
    "The `departamento` column is a critical identifier. A row without a department is like a letter without an address: useless. We cannot guess which department it belongs to, so we remove those rows.\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_before = len(df)\n",
    "\n",
    "df = df.dropna(subset=['departamento'])\n",
    "\n",
    "rows_after = len(df)\n",
    "print(f\"Rows before: {rows_before}\")\n",
    "print(f\"Rows after:  {rows_after}\")\n",
    "print(f\"Removed:     {rows_before - rows_after} rows (missing departamento)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Fill Rate Columns with the Median\n",
    "\n",
    "For columns that represent rates or percentages (dropout rate, coverage, approval, etc.), we fill with the **median**. The median is the middle value when all values are sorted. It is better than the mean because it is not distorted by extreme outliers.\n",
    "\n",
    "**Why not fill with 0?** A 0% dropout rate means \"nobody dropped out.\" That is very different from \"we don't know.\" The median says: \"if we had to guess, the most typical value is probably close to this.\"\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_columns = [\n",
    "    'tasa_matriculacion_5_16',\n",
    "    'cobertura_neta', 'cobertura_neta_transicion', 'cobertura_neta_primaria',\n",
    "    'cobertura_neta_secundaria', 'cobertura_neta_media',\n",
    "    'cobertura_bruta', 'cobertura_bruta_transicion', 'cobertura_bruta_primaria',\n",
    "    'cobertura_bruta_secundaria', 'cobertura_bruta_media',\n",
    "    'desercion', 'desercion_transicion', 'desercion_primaria',\n",
    "    'desercion_secundaria', 'desercion_media',\n",
    "    'aprobacion', 'aprobacion_transicion', 'aprobacion_primaria',\n",
    "    'aprobacion_secundaria', 'aprobacion_media',\n",
    "    'reprobacion', 'reprobacion_transicion', 'reprobacion_primaria',\n",
    "    'reprobacion_secundaria', 'reprobacion_media',\n",
    "    'repitencia', 'repitencia_transicion', 'repitencia_primaria',\n",
    "    'repitencia_secundaria', 'repitencia_media',\n",
    "]\n",
    "\n",
    "for col in rate_columns:\n",
    "    n_missing = df[col].isnull().sum()\n",
    "    if n_missing > 0:\n",
    "        median_val = df[col].median()\n",
    "        df[col] = df[col].fillna(median_val)\n",
    "        print(f\"{col}: filled {n_missing} NaN with median {median_val:.2f}\")\n",
    "\n",
    "print(f\"\\nTotal NaN remaining in rate columns: {df[rate_columns].isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened:** Each rate column's missing values were replaced with that column's median. For example, if the median dropout rate is 3.86%, all missing dropout values now say 3.86%. This is a reasonable assumption: \"if we don't know the value, assume it is typical.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Verify: How Many Missing Values Remain?\n",
    "\n",
    "Let's check how our cleaning is going so far.\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining = df.isnull().sum()\n",
    "remaining = remaining[remaining > 0]\n",
    "\n",
    "if len(remaining) == 0:\n",
    "    print(\"No missing values remain!\")\n",
    "else:\n",
    "    print(f\"Columns still with missing values: {len(remaining)}\")\n",
    "    print(remaining)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What to expect:** The only column with missing values should be `poblacion_5_16`. We will fix it in Section 3, because its problem is not just missing values but also wrong data types (it has commas and the text \"sin dato\").\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 2\n",
    "\n",
    "Why would filling `desercion` (dropout rate) with 0 be misleading? What does 0% dropout actually mean vs. \"no data\"?\n",
    "\n",
    "*Double-click this cell and write your answer below:*\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Data Type Issues\n",
    "\n",
    "Imagine sorting your laundry and finding **a shoe in the shirt pile**. It does not belong there, and it will cause problems in the wash. That is what happens when a number is stored as text: pandas cannot do math with it.\n",
    "\n",
    "There are three main types we care about:\n",
    "- `int64`: whole numbers (years, counts)\n",
    "- `float64`: decimal numbers (rates, percentages)\n",
    "- `object`: text/strings (names, categories)\n",
    "\n",
    "We found two type problems:\n",
    "1. `ano` (year): stored as `float64` (2023.0) instead of `int64` (2023)\n",
    "2. `poblacion_5_16` (population): stored as `object` (text) instead of `int64`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Inspect `ano`\n",
    "\n",
    "Let's see what the `ano` column looks like right now.\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"dtype: {df['ano'].dtype}\")\n",
    "print(f\"\\nSample values: {df['ano'].head(10).tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Fix `ano`: Convert to Integer\n",
    "\n",
    "Years should be whole numbers. The CSV had mixed formats like \"2011\" and \"2011.0\", which caused pandas to read the column as float. We fix it by first ensuring all values are numeric with `pd.to_numeric()`, then converting to int.\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Before: ano dtype = {df['ano'].dtype}\")\n",
    "print(f\"Sample: {df['ano'].head(5).tolist()}\")\n",
    "\n",
    "df['ano'] = pd.to_numeric(df['ano'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "print(f\"\\nAfter: ano dtype = {df['ano'].dtype}\")\n",
    "print(f\"Sample: {df['ano'].head(5).tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened:** The years changed from 2023.0 (float) to 2023 (integer). Clean and correct.\n",
    "\n",
    "**The NaN-before-int trap:** If `ano` had NaN values and we tried `astype(int)` directly, pandas would crash. The `fillna(0)` step prevents that. Always fill NaN before converting to int."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Inspect `poblacion_5_16`\n",
    "\n",
    "This column is text (`object`) instead of numeric. Let's look at the raw values to understand why.\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Current dtype: {df['poblacion_5_16'].dtype}\")\n",
    "print(f\"\\nSample values (10 random):\")\n",
    "print(df['poblacion_5_16'].dropna().sample(10, random_state=42).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What to notice:** Some values have commas (like \"394,574\") and some say \"sin dato\" (Spanish for \"no data\"). Pandas cannot convert these to numbers automatically, so it stored the whole column as text.\n",
    "\n",
    "### 3.4 Fix `poblacion_5_16`: Clean String, Then Convert\n",
    "\n",
    "The fix is a 2-step process:\n",
    "1. **Remove the commas** with `str.replace(',', '')`\n",
    "2. **Convert to numeric** with `pd.to_numeric(errors='coerce')` so \"sin dato\" becomes NaN instead of crashing\n",
    "\n",
    "Then we fill the remaining NaN with 0 and convert to int.\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Remove commas, then convert to numeric\n",
    "df['poblacion_5_16'] = pd.to_numeric(\n",
    "    df['poblacion_5_16'].astype(str).str.replace(',', ''),\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "print(f\"After to_numeric: dtype = {df['poblacion_5_16'].dtype}\")\n",
    "print(f\"NaN count: {df['poblacion_5_16'].isnull().sum()}\")\n",
    "\n",
    "# Step 2: Fill NaN with 0 and convert to int\n",
    "df['poblacion_5_16'] = df['poblacion_5_16'].fillna(0).astype(int)\n",
    "\n",
    "print(f\"\\nFinal dtype: {df['poblacion_5_16'].dtype}\")\n",
    "print(f\"NaN remaining: {df['poblacion_5_16'].isnull().sum()}\")\n",
    "print(f\"Sample: {df['poblacion_5_16'].head(5).tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened:**\n",
    "- Commas were removed: \"394,574\" became \"394574\"\n",
    "- `pd.to_numeric()` converted valid numbers to float64\n",
    "- \"sin dato\" and NaN values became NaN (the `errors='coerce'` flag replaces anything it cannot convert with NaN, instead of crashing)\n",
    "- Finally, we filled NaN with 0 and converted to int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR TURN 1\n",
    "\n",
    "Verify the type fixes worked. Write code to print the `dtype` and 5 sample values for both `ano` and `poblacion_5_16`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: print dtype and sample values for 'ano' and 'poblacion_5_16'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Duplicates\n",
    "\n",
    "Imagine you are folding laundry and you count the **same shirt twice**. Your count is now wrong. That is exactly what duplicate rows do: they inflate counts and distort averages. If a department appears twice for the same year with identical data, every calculation using that data is biased.\n",
    "\n",
    "### 4.1 Count Duplicates\n",
    "\n",
    "We use `duplicated().sum()` to count how many duplicate rows exist.\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dupes = df.duplicated().sum()\n",
    "print(f\"Duplicate rows found: {n_dupes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 See the Duplicates\n",
    "\n",
    "Let's look at the actual duplicate rows. Using `keep=False` marks ALL copies (both the \"original\" and the \"duplicate\") so we can see them side by side.\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dupes = df[df.duplicated(keep=False)].sort_values(['departamento', 'ano'])\n",
    "print(f\"Total rows involved in duplicates: {len(dupes)}\")\n",
    "print()\n",
    "print(dupes[['ano', 'departamento', 'poblacion_5_16', 'desercion']].head(20).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Remove Duplicates\n",
    "\n",
    "`drop_duplicates()` keeps the first occurrence of each row and removes the rest.\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_before = len(df)\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "rows_after = len(df)\n",
    "print(f\"Rows before: {rows_before}\")\n",
    "print(f\"Rows after:  {rows_after}\")\n",
    "print(f\"Removed:     {rows_before - rows_after} duplicate rows\")\n",
    "print(f\"Duplicates remaining: {df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened:** The duplicate rows were removed. We kept one copy of each and deleted the rest. The 20 duplicates were exact copies injected into the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 3\n",
    "\n",
    "When might a duplicate row be **valid** and NOT an error? Give one example from the real world.\n",
    "\n",
    "*Double-click this cell and write your answer below:*\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Text Inconsistencies\n",
    "\n",
    "Imagine you are folding laundry but your labels are **inconsistent**: one shirt says \"Blue\", another says \"BLUE\", another says \"  blue  \". To you, they are the same color. But to pandas, these are three completely different values. This breaks any grouping or counting operation.\n",
    "\n",
    "### 5.1 Explore the Problem\n",
    "\n",
    "Colombia has about 34 departments (32 + Bogota D.C. + national aggregate). Let's see how many unique values our `departamento` column actually has.\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Unique department values: {df['departamento'].nunique()}\")\n",
    "print(f\"(We expect about 34)\")\n",
    "print(f\"\\nAll unique values (sorted):\")\n",
    "for val in sorted(df['departamento'].unique()):\n",
    "    print(f\"  '{val}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened:** We see far more unique values than expected. The same department appears in multiple forms:\n",
    "- \"Antioquia\", \"ANTIOQUIA\", \"  Antioquia  \", \"antioquia\" are all the same department\n",
    "- Some have leading/trailing spaces\n",
    "- Some have accents stripped (\"Narino\" vs \"NariÃ±o\")\n",
    "\n",
    "To pandas, every single variation is a completely different string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Standardize Text\n",
    "\n",
    "The fix: convert everything to uppercase and remove extra whitespace with `str.upper().str.strip()`.\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_nunique = df['departamento'].nunique()\n",
    "\n",
    "df['departamento'] = df['departamento'].str.upper().str.strip()\n",
    "\n",
    "after_nunique = df['departamento'].nunique()\n",
    "\n",
    "print(f\"Unique values before: {before_nunique}\")\n",
    "print(f\"Unique values after:  {after_nunique}\")\n",
    "print(f\"Reduced by:           {before_nunique - after_nunique} values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened:** All department names were converted to uppercase and extra spaces were removed. The number of unique values dropped significantly. Now \"Antioquia\", \"ANTIOQUIA\", \"  antioquia  \" are all just \"ANTIOQUIA\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Check for New Duplicates\n",
    "\n",
    "Here is a subtle but important lesson: **cleaning one thing can reveal new problems.** Rows that looked different before (\"Antioquia\" vs \"ANTIOQUIA\" for the same year) are now identical after uppercasing. We need to check for duplicates again.\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dupes = df.duplicated().sum()\n",
    "print(f\"New duplicates after text standardization: {new_dupes}\")\n",
    "\n",
    "if new_dupes > 0:\n",
    "    rows_before = len(df)\n",
    "    df = df.drop_duplicates()\n",
    "    rows_after = len(df)\n",
    "    print(f\"Removed {rows_before - rows_after} additional duplicates\")\n",
    "    print(f\"Final row count: {rows_after}\")\n",
    "else:\n",
    "    print(\"No new duplicates created. Good.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lesson:** Data cleaning is iterative. Fixing one problem (text inconsistency) can create another (new duplicates). Always verify after each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR TURN 2\n",
    "\n",
    "After standardization, verify how many unique departments we have now. Print the sorted list of unique department names. Does the count look reasonable for Colombia?\n",
    "\n",
    "Write your code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: print nunique() and sorted list of unique departments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Invalid Values\n",
    "\n",
    "So far we have fixed missing values, wrong types, duplicates, and text inconsistencies. But there is one more problem hiding in the data: **values that exist but are impossible.**\n",
    "\n",
    "Think of it this way: you finished your laundry, everything is folded and sorted. But then you notice a shirt labeled \"Size -3\" and another labeled \"Size 250.\" Those sizes do not exist. The labels are wrong.\n",
    "\n",
    "In our dataset, percentage columns (dropout rate, coverage, approval, etc.) must be between 0 and 100. A dropout rate of -5% is impossible. A coverage of 150% is impossible (for net coverage). These values passed all our previous checks because they are not missing, they are not the wrong type, and they are not duplicates. **They are just wrong.**\n",
    "\n",
    "Catching these requires **domain knowledge**: knowing what valid values look like for your specific data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Check Min and Max of Percentage Columns\n",
    "\n",
    "Let's use `describe()` on just the percentage columns to check their minimum and maximum values. Any `min` below 0 or `max` above 100 is suspicious.\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_cols = [\n",
    "    'desercion', 'desercion_primaria', 'desercion_secundaria', 'desercion_media',\n",
    "    'cobertura_neta', 'cobertura_neta_primaria', 'cobertura_neta_secundaria', 'cobertura_neta_media',\n",
    "    'aprobacion', 'reprobacion',\n",
    "]\n",
    "\n",
    "summary = df[percentage_cols].describe().loc[['min', 'max']].round(2)\n",
    "print(\"Min and Max for percentage columns:\")\n",
    "print(summary.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened:** Look at the `min` row. Do you see any negative values? Now look at the `max` row. Do you see anything above 100? Those are invalid values that should not exist in percentage columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Count the Invalid Values\n",
    "\n",
    "Let's count exactly how many values are below 0 and above 100 in each column.\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives = df[percentage_cols].lt(0).sum()\n",
    "over_100 = df[percentage_cols].gt(100).sum()\n",
    "\n",
    "print(\"Negative values (< 0) per column:\")\n",
    "print(negatives[negatives > 0].to_string())\n",
    "print(f\"\\nTotal negative values: {negatives.sum()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "\n",
    "print(\"\\nValues over 100 per column:\")\n",
    "print(over_100[over_100 > 0].to_string())\n",
    "print(f\"\\nTotal values over 100: {over_100.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 See the Invalid Rows\n",
    "\n",
    "Let's look at the actual rows with invalid values so we can understand the scope of the problem.\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find rows with any negative percentage value\n",
    "mask_negative = (df[percentage_cols] < 0).any(axis=1)\n",
    "mask_over_100 = (df[percentage_cols] > 100).any(axis=1)\n",
    "\n",
    "print(f\"Rows with negative values: {mask_negative.sum()}\")\n",
    "if mask_negative.sum() > 0:\n",
    "    print(df.loc[mask_negative, ['ano', 'departamento'] + percentage_cols].to_string())\n",
    "\n",
    "print(f\"\\nRows with values > 100: {mask_over_100.sum()}\")\n",
    "if mask_over_100.sum() > 0:\n",
    "    print(df.loc[mask_over_100, ['ano', 'departamento'] + percentage_cols].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Fix Invalid Values\n",
    "\n",
    "We will replace invalid values with NaN, then fill them with the column median (the same strategy we used for missing values). This is the safer approach: we treat impossible values the same as missing data.\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_fixed = 0\n",
    "\n",
    "for col in percentage_cols:\n",
    "    # Count invalid values\n",
    "    invalid_mask = (df[col] < 0) | (df[col] > 100)\n",
    "    n_invalid = invalid_mask.sum()\n",
    "    \n",
    "    if n_invalid > 0:\n",
    "        # Replace invalid with NaN\n",
    "        df.loc[invalid_mask, col] = np.nan\n",
    "        \n",
    "        # Fill with median\n",
    "        median_val = df[col].median()\n",
    "        df[col] = df[col].fillna(median_val)\n",
    "        \n",
    "        print(f\"{col}: fixed {n_invalid} invalid values (replaced with median {median_val:.2f})\")\n",
    "        total_fixed += n_invalid\n",
    "\n",
    "print(f\"\\nTotal invalid values fixed: {total_fixed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened:** We found values that were mathematically present but logically impossible (negative percentages, percentages above 100). We replaced them with NaN and then filled with the median, just like we do with missing values.\n",
    "\n",
    "**Key takeaway:** Domain knowledge is essential for data cleaning. Without knowing that dropout rates must be between 0 and 100, we would never catch these errors. The `describe()` function is your friend here: always check `min` and `max` against what you know about the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Verify the Fix\n",
    "\n",
    "Let's confirm that all percentage values are now within the valid 0-100 range.\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After fix - Min and Max for percentage columns:\")\n",
    "print(df[percentage_cols].describe().loc[['min', 'max']].round(2).to_string())\n",
    "\n",
    "remaining_invalid = (df[percentage_cols].lt(0).sum().sum() + df[percentage_cols].gt(100).sum().sum())\n",
    "print(f\"\\nInvalid values remaining: {remaining_invalid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 4\n",
    "\n",
    "Which approach is better for invalid percentage values:\n",
    "\n",
    "**(a)** Replace with NaN and fill with the median (what we did), or  \n",
    "**(b)** Clip to the valid range (set negatives to 0 and values >100 to 100)?\n",
    "\n",
    "Think about what each approach assumes. When might clipping be better? When might the median approach be better?\n",
    "\n",
    "*Double-click this cell and write your answer below:*\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7: Summary\n",
    "\n",
    "We have completed all 5 cleaning steps. Let's compare the original dataset with our cleaned version to see the full impact of our work.\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 55)\n",
    "print(\"  BEFORE CLEANING (original)\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"  Rows:                {len(df_original)}\")\n",
    "print(f\"  Total NaN:           {df_original.isnull().sum().sum()}\")\n",
    "print(f\"  Duplicates:          {df_original.duplicated().sum()}\")\n",
    "print(f\"  Unique departments:  {df_original['departamento'].nunique()}\")\n",
    "print(f\"  ano dtype:           {df_original['ano'].dtype}\")\n",
    "print(f\"  poblacion dtype:     {df_original['poblacion_5_16'].dtype}\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"=\" * 55)\n",
    "print(\"  AFTER CLEANING\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"  Rows:                {len(df)}\")\n",
    "print(f\"  Total NaN:           {df.isnull().sum().sum()}\")\n",
    "print(f\"  Duplicates:          {df.duplicated().sum()}\")\n",
    "print(f\"  Unique departments:  {df['departamento'].nunique()}\")\n",
    "print(f\"  ano dtype:           {df['ano'].dtype}\")\n",
    "print(f\"  poblacion dtype:     {df['poblacion_5_16'].dtype}\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 55)\n",
    "print(\"  INVALID VALUES CHECK\")\n",
    "print(\"=\" * 55)\n",
    "pct_cols_check = ['desercion', 'cobertura_neta', 'aprobacion', 'reprobacion']\n",
    "for col in pct_cols_check:\n",
    "    print(f\"  {col}: min={df[col].min():.2f}, max={df[col].max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 5-Step Cleaning Workflow\n",
    "\n",
    "Here is the workflow you just practiced. Use these exact steps in every data project:\n",
    "\n",
    "| Step | What | Key Commands |\n",
    "|------|------|--------------|\n",
    "| 1. **Inspect** | Understand the data before touching it | `df.shape`, `df.dtypes`, `df.isnull().sum()`, `df.describe()` |\n",
    "| 2. **Handle missing** | Decide: drop, fill with 0, or fill with median | `fillna(0)`, `fillna(median)`, `dropna(subset=...)` |\n",
    "| 3. **Fix types** | Numbers stored as text, floats that should be int | `pd.to_numeric(errors='coerce')`, `astype(int)` |\n",
    "| 4. **Remove duplicates** | Exact copies that inflate counts | `duplicated().sum()`, `drop_duplicates()` |\n",
    "| 5. **Validate values** | Impossible values based on domain knowledge | `describe()` min/max, boolean masks |\n",
    "\n",
    "**Remember:** Cleaning is iterative. Fixing one problem can create another (like text standardization creating new duplicates). Always verify after each step.\n",
    "\n",
    "This dataset is now ready for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FINAL REFLECTION\n",
    "\n",
    "Write 2-3 sentences:\n",
    "\n",
    "1. What was the most surprising thing you found during cleaning?\n",
    "2. Which of the 5 steps do you think is most important? Why?\n",
    "3. How would you apply these steps to your project dataset from datos.gov.co?\n",
    "\n",
    "*Double-click this cell and write your answer below:*\n",
    "\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}