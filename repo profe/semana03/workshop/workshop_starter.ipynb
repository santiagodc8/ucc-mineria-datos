{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Week 3 Workshop: Data Cleaning Practice\n\n**Duration:** ~2 hours (independent work)  \n**Dataset:** Health Indicators - Mortality and Morbidity from the Colombian Ministry of Health (datos.gov.co)  \n**Rows:** 532 (dirty) | **Columns:** 15  \n\n---\n\n### What is this?\n\nThis is your **independent practice** notebook. Unlike the in-class exercise where code was provided for you to run,\nhere **you write all the code yourself**. Each section tells you what to do, gives you hints about which pandas\nmethods to use, and describes what your output should look like. But the actual code is yours to write.\n\n### How to work through this notebook\n\n1. **Read** the markdown cell explaining the task\n2. **Write** your code in the code cell (follow the comments for structure)\n3. **Run** your code and compare with the expected output described\n4. **Document** your decisions in the reflection cells (these matter for grading)\n\n### Grading\n\nYou are graded on **two things equally**:\n- Correct, working code that cleans the dataset\n- Thoughtful documentation of your decisions (the markdown cells asking for your reasoning)\n\n### The 5 data quality issues you must fix\n\n| # | Issue | Columns affected |\n|---|-------|------------------|\n| 1 | Missing values (NaN) | Multiple indicator columns, `departamento` |\n| 2 | Wrong data types | `ano` (float instead of int), `cod_municipio` (text with commas and \"sin dato\") |\n| 3 | Duplicate rows | ~20 exact duplicates |\n| 4 | Text inconsistencies | `departamento` has ~87 unique values instead of ~22 |\n| 5 | Invalid values | Negative percentages and percentages over 100 in `bajo_peso_nacer`, `partos_cesarea`, `partos_institucionales` |\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup\n\nRun this cell as-is. It imports the libraries, loads the dataset, and stores a copy of the original\nso you can compare before and after at the end."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\n\ndf = pd.read_csv('../data/indicadores_salud.csv')\ndf_original = df.copy()\n\nprint(f\"Dataset loaded: {df.shape[0]} rows x {df.shape[1]} columns\")\nprint(f\"\\nColumns: {df.columns.tolist()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Part 1: Initial Inspection\n\nBefore you clean anything, you need to **diagnose** the problems. This is the inspection ritual\nyou should run at the start of every data project:\n\n1. `df.shape` -- how big is the dataset?\n2. `df.head()` -- what does the actual data look like?\n3. `df.dtypes` -- are columns the right type?\n4. `df.isnull().sum()` -- where are the gaps?\n5. `df.describe()` -- are the numbers reasonable?\n\nRun all five in the cell below. The first line is done for you. Complete the rest."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 1. Shape\nprint(f\"Rows: {df.shape[0]}, Columns: {df.shape[1]}\")\nprint(\"=\" * 50)\n\n# 2. First rows -- display the first 5 rows\n# YOUR CODE\n\nprint(\"=\" * 50)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 3. Data types -- display all column types\n# Look for: ano should be int but is float. cod_municipio should be numeric but is object.\n# YOUR CODE"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 4. Missing values -- count NaN per column, sorted from worst to least\n# Hint: df.isnull().sum().sort_values(ascending=False)\n# Only show columns that have at least 1 missing value\n# YOUR CODE"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 5. Statistical summary -- run describe() and look at min/max values\n# Are there any negative percentages? Any values over 100 that shouldn't be?\n# YOUR CODE"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Documentation: Initial Inspection\n\nBased on the 5 commands you just ran, list the data quality issues you found.\nFor each one, note which column(s) are affected, what the problem is, and which command revealed it.\n\n| # | Issue | Column(s) | How you spotted it |\n|---|-------|-----------|--------------------|\n| 1 | *YOUR ANSWER* | *YOUR ANSWER* | *YOUR ANSWER* |\n| 2 | *YOUR ANSWER* | *YOUR ANSWER* | *YOUR ANSWER* |\n| 3 | *YOUR ANSWER* | *YOUR ANSWER* | *YOUR ANSWER* |\n| 4 | *YOUR ANSWER* | *YOUR ANSWER* | *YOUR ANSWER* |\n| 5 | *YOUR ANSWER* | *YOUR ANSWER* | *YOUR ANSWER* |\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Part 2: Missing Values\n\nMissing values (NaN) prevent calculations and distort results. But not all missing values\nshould be handled the same way. The strategy depends on **what the column represents**\nand **how much data is missing**.\n\nUse this decision framework:\n\n| Missing % | Recommended action | Reasoning |\n|-----------|-------------------|-----------|\n| > 50% | Consider dropping the column, or fill with 0 if \"not reported\" makes sense | More gaps than data |\n| < 5% | Drop the rows | Losing very few rows |\n| 5-50% | Fill with an appropriate value (median for rates, 0 for counts) | Too many rows to lose |\n\nYou need to handle 3 groups of missing values:\n\n1. **`mortalidad_materna`**: High percentage of NaN. Maternal mortality is rare in many municipalities, so \"not reported\" effectively means 0 deaths. Fill with 0.\n2. **`departamento`**: ~3% missing. This is the key identifier. Drop those rows (you cannot guess which department a row belongs to).\n3. **Indicator columns** (mortality rates, birth indicators, etc.): ~8-12% missing. Fill with the median (the middle value, resistant to outliers).\n\n**Why median instead of mean for rates?** The mean is pulled by extreme outliers. The median is the\nmiddle value and better represents \"typical.\" For health indicators, median is the safer assumption.\n\n**Why NOT fill rates with 0?** A 0 infant mortality rate means \"no infants died\" (a strong claim).\nThat is very different from \"we don't know.\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Step 2.1: Calculate missing percentages\n\nCalculate the percentage of missing values per column. Show only columns with at least 1 missing value,\nsorted from highest to lowest.\n\n**Hint:** `(df.isnull().sum() / len(df) * 100).round(2)`\n\n**Expected output:** You should see `mortalidad_materna` with the highest percentage,\n`departamento` around 2-3%, and several indicator columns around 8-12%."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Calculate percentage of missing values per column\nmissing_pct = # YOUR CODE: (df.isnull().sum() / len(df) * 100).round(2)\n\n# Show only columns with missing values, sorted descending\n# YOUR CODE: filter to only > 0, sort descending, print"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Step 2.2: Fill `mortalidad_materna` with 0\n\nThe column `mortalidad_materna` (maternal mortality ratio per 100,000 live births) has a high\npercentage of NaN. Maternal deaths are rare events, especially in small municipalities.\nWhen a municipality reports no data, it typically means zero maternal deaths occurred.\nFill with 0 to indicate \"no maternal deaths reported.\"\n\n**Hint:** `df['column'] = df['column'].fillna(0)`\n\n**Expected output:** `mortalidad_materna` should go from many NaN to 0 NaN."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Print NaN count before\nprint(\"Before:\")\nprint(f\"  mortalidad_materna NaN: {df['mortalidad_materna'].isnull().sum()}\")\n\n# YOUR CODE: fill mortalidad_materna with 0\n\n\n# Print NaN count after\nprint(\"\\nAfter:\")\nprint(f\"  mortalidad_materna NaN: {df['mortalidad_materna'].isnull().sum()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Step 2.3: Drop rows with missing `departamento`\n\nThe `departamento` column is the key identifier. A row without a department is like a letter\nwithout an address: we cannot use it. Drop those rows.\n\n**Hint:** `df = df.dropna(subset=['departamento'])`\n\n**Expected output:** You should remove approximately 10-15 rows."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "rows_before = len(df)\n\n# YOUR CODE: drop rows where departamento is NaN\n\n\nrows_after = len(df)\nprint(f\"Rows before: {rows_before}\")\nprint(f\"Rows after:  {rows_after}\")\nprint(f\"Removed {rows_before - rows_after} rows with missing departamento\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Step 2.4: Fill indicator columns with the median\n\nIndicator columns represent health rates (mortality, fertility, birth weight, etc.). For these,\nfilling with the median is the safest strategy: \"if we don't know, assume a typical value.\"\n\nThe list of indicator columns is provided below. Note that `mortalidad_materna` was already\nhandled in Step 2.2, so it is excluded from this list. Loop through each column, check if\nit has missing values, and fill them with that column's median.\n\n**Hint:** For each column, use `df[col].fillna(df[col].median())` to fill NaN with the median.\n\n**Expected output:** Each column should print how many NaN were filled and what median was used.\nAfter the loop, all indicator columns should have 0 NaN."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "indicator_columns = [\n    'bajo_peso_nacer', 'controles_prenatales', 'fecundidad_adolescente',\n    'mortalidad_fetal', 'mortalidad_general', 'mortalidad_infantil',\n    'mortalidad_neonatal',\n    'partos_cesarea', 'partos_institucionales',\n]\n\n# YOUR CODE: loop through indicator_columns\n#   For each column:\n#     1. Count how many NaN it has\n#     2. If it has any NaN, calculate the median\n#     3. Fill NaN with the median\n#     4. Print: \"{column}: filled {n} NaN with median {value:.2f}\"\n\n\n\n# Verify: total NaN remaining in indicator columns\nprint(f\"\\nTotal NaN remaining in indicator columns: {df[indicator_columns].isnull().sum().sum()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Step 2.5: Verify missing values\n\nCheck how many total NaN remain in the entire dataset.\n\n**Expected output:** The only column with remaining NaN should be `cod_municipio` (we fix\nthat in Part 3, because it has both missing values AND type problems)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE: show remaining NaN per column (only columns with > 0)\nremaining_nan = df.isnull().sum()\nremaining_nan = remaining_nan[remaining_nan > 0]\n\nif len(remaining_nan) == 0:\n    print(\"No missing values remain!\")\nelse:\n    print(f\"Columns still with NaN ({len(remaining_nan)}):\")\n    print(remaining_nan)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Documentation: Missing Values\n\nAnswer these questions:\n\n**1. Which fill strategy did you use for each type of column?**\n\n| Column type | Strategy | Why |\n|-------------|----------|-----|\n| mortalidad_materna | *YOUR ANSWER* | *YOUR ANSWER* |\n| departamento | *YOUR ANSWER* | *YOUR ANSWER* |\n| Indicator columns | *YOUR ANSWER* | *YOUR ANSWER* |\n\n**2. Why did you choose median over mean for indicator columns?**\n\n*YOUR ANSWER*\n\n**3. How many missing values did `departamento` have? Why is dropping rows acceptable here but not for indicator columns?**\n\n*YOUR ANSWER*\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Part 3: Data Type Issues\n\nEven if a column contains numbers, pandas might store it as text (`object`) if even one value\nhas non-numeric characters. And a column of whole numbers will be stored as `float64` (with\ndecimal points) if it contains any NaN values, because NaN is technically a float.\n\nYou need to fix two columns:\n\n1. **`ano`** (year): Currently `float64` (shows as 2015.0). Should be `int64` (2015).\n2. **`cod_municipio`** (municipality code): Currently `object` (text) because some values have commas like \"05,001\" and some say \"sin dato\". Should be `int64`.\n\n**Key methods:**\n- `pd.to_numeric(series, errors='coerce')` -- converts to number, turns unparseable values into NaN\n- `.str.replace(',', '')` -- removes commas from strings\n- `.fillna(0).astype(int)` -- fills NaN then converts to integer\n\n**Why `errors='coerce'`?** Without it, `pd.to_numeric()` crashes when it hits a non-numeric\nvalue like \"sin dato\". With `errors='coerce'`, it quietly turns those into NaN instead."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Step 3.1: Fix `ano` (year)\n\nConvert `ano` from float to integer. The process is:\n1. Use `pd.to_numeric(errors='coerce')` to handle any non-numeric values\n2. Fill remaining NaN with 0\n3. Convert to int with `.astype(int)`\n\n**Expected output:** `ano` dtype changes from `float64` to `int64`. Sample values like 2015.0 become 2015."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(f\"Before: ano dtype = {df['ano'].dtype}\")\nprint(f\"Sample: {df['ano'].head(5).tolist()}\")\n\n# YOUR CODE: convert ano to int\n#   1. pd.to_numeric() with errors='coerce'\n#   2. fillna(0)\n#   3. astype(int)\n\n\nprint(f\"\\nAfter: ano dtype = {df['ano'].dtype}\")\nprint(f\"Sample: {df['ano'].head(5).tolist()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Step 3.2: Fix `cod_municipio` (municipality code)\n\nThis column is trickier. It is stored as text (`object`) because:\n- Some values have commas: \"05,001\"\n- Some values say \"sin dato\" (\"no data\" in Spanish)\n\nThe process is:\n1. Convert to string with `.astype(str)` (ensures all values are strings for `.str.replace()`)\n2. Remove commas with `.str.replace(',', '')`\n3. Convert to numeric with `pd.to_numeric(errors='coerce')` (\"sin dato\" becomes NaN)\n4. Fill NaN with 0 and convert to int\n\n**Expected output:** `cod_municipio` dtype changes from `object` to `int64`. Values like \"05,001\" become 5001."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(f\"Before: cod_municipio dtype = {df['cod_municipio'].dtype}\")\nprint(f\"Sample: {df['cod_municipio'].head(5).tolist()}\")\n\n# YOUR CODE: convert cod_municipio to int\n#   1. .astype(str) to make sure everything is a string\n#   2. .str.replace(',', '') to remove commas\n#   3. pd.to_numeric(errors='coerce') to convert (\"sin dato\" becomes NaN)\n#   4. .fillna(0).astype(int)\n\n\nprint(f\"\\nAfter: cod_municipio dtype = {df['cod_municipio'].dtype}\")\nprint(f\"Sample: {df['cod_municipio'].head(5).tolist()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Step 3.3: Verify type fixes\n\nPrint the dtypes of `ano` and `cod_municipio` to confirm they are both `int64`.\n\n**Expected output:** Both should be `int64`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE: print dtype for both columns and verify they are int64\nprint(f\"ano:            {df['ano'].dtype}\")\nprint(f\"cod_municipio:  {df['cod_municipio'].dtype}\")\n\n# Also check: any remaining NaN in these columns?\nprint(f\"\\nano NaN:            {df['ano'].isnull().sum()}\")\nprint(f\"cod_municipio NaN:  {df['cod_municipio'].isnull().sum()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Documentation: Data Types\n\nAnswer these questions:\n\n**1. What would happen if you tried `df['ano'].astype(int)` directly on a column that still has NaN values?**\n\n*YOUR ANSWER*\n\n**2. Why do we need `errors='coerce'` in `pd.to_numeric()`? What would happen without it when the column contains \"sin dato\"?**\n\n*YOUR ANSWER*\n\n**3. Why do we convert to string first (`.astype(str)`) before using `.str.replace()`?**\n\n*YOUR ANSWER*\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Part 4: Duplicate Rows\n\nDuplicate rows inflate counts and distort averages. If a municipality appears twice for the\nsame year with identical data, every calculation using that data is biased. The dataset\nshould have ~512 rows (one per municipality per year), but it has more because of duplicates.\n\n**Key methods:**\n- `df.duplicated().sum()` -- count how many duplicate rows exist\n- `df[df.duplicated(keep=False)]` -- show ALL copies (both \"original\" and \"duplicate\")\n- `df.drop_duplicates()` -- keep the first occurrence, remove the rest\n\n**Expected result:** You should find approximately 20 duplicate rows. After removal, expect ~512 rows."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Step 4.1: Count duplicates\n\nHow many exact duplicate rows are in the dataset?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE: count duplicate rows\nn_dupes = # YOUR CODE\nprint(f\"Duplicate rows: {n_dupes}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Step 4.2: Examine the duplicates\n\nLook at the actual duplicate rows to understand what they are. Use `keep=False` to see\nboth the original and the copy side by side. Sort by `departamento` and `ano` to group them.\n\nShow only the key columns: `ano`, `departamento`, `municipio`, `mortalidad_general`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE: show all duplicate rows, sorted by departamento and ano\n# Display columns: ano, departamento, municipio, mortalidad_general\n# Hint: df[df.duplicated(keep=False)].sort_values([...])[[...]]\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Step 4.3: Remove duplicates\n\nRemove the duplicate rows, keeping the first occurrence of each."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "rows_before = len(df)\n\n# YOUR CODE: remove duplicates\n\n\nrows_after = len(df)\nprint(f\"Rows before: {rows_before}\")\nprint(f\"Rows after:  {rows_after}\")\nprint(f\"Removed {rows_before - rows_after} duplicate rows\")\nprint(f\"Duplicates remaining: {df.duplicated().sum()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Documentation: Duplicates\n\nAnswer these questions:\n\n**1. How many duplicate rows did you find? Are they exact copies (all columns identical)?**\n\n*YOUR ANSWER*\n\n**2. When might a duplicate row be valid and NOT an error? Give one real-world example.**\n\n*YOUR ANSWER*\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Part 5: Text Inconsistencies\n\nColombia has about 22 departments represented in this dataset. But our `departamento` column\nhas far more unique values because the same department appears in different forms:\n\n- \"Antioquia\", \"ANTIOQUIA\", \"  Antioquia  \", \"antioquia\"\n- Leading/trailing spaces\n- Mixed upper/lower case\n\nTo pandas, each of these is a completely different string. This breaks grouping: if you\ntry `df.groupby('departamento')`, you get ~87 groups instead of ~22.\n\n**Key methods:**\n- `.str.upper()` -- convert to uppercase\n- `.str.strip()` -- remove leading/trailing whitespace\n- `.nunique()` -- count unique values\n\n**IMPORTANT:** After standardizing text, previously different rows (\"Antioquia\" and \"ANTIOQUIA\"\nfor the same year and municipality) become identical. You must check for NEW duplicates after this step.\n\n**Expected result:** Unique departments should drop from ~87 to ~22."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Step 5.1: Check current state\n\nHow many unique department values exist before standardization? Print the count and the\nsorted list of all unique values."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(f\"Unique departments before: {df['departamento'].nunique()}\")\nprint(f\"(Expected: ~22)\")\nprint()\n\n# YOUR CODE: print sorted list of all unique department values\n# Hint: sorted(df['departamento'].unique())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Step 5.2: Standardize text\n\nApply `.str.upper().str.strip()` to the `departamento` column."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE: standardize departamento to uppercase and stripped\n\n\nprint(f\"Unique departments after: {df['departamento'].nunique()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Step 5.3: Check for NEW duplicates\n\nCleaning text can reveal new problems. Rows that looked different before (\"Antioquia\" vs\n\"ANTIOQUIA\" for the same year and municipality) are now identical. Check for and remove any new duplicates."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE: check for new duplicates\nnew_dupes = # YOUR CODE\nprint(f\"New duplicates after text fix: {new_dupes}\")\n\n# YOUR CODE: if there are new duplicates, remove them\n\n\nprint(f\"Final row count: {len(df)}\")\nprint(f\"Duplicates remaining: {df.duplicated().sum()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Step 5.4: Verify final department list\n\nPrint the sorted list of unique departments. It should look like a clean list of\nColombian departments, all uppercase, no duplicates."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE: print sorted unique departments after cleaning\nprint(f\"Unique departments: {df['departamento'].nunique()}\")\nprint()\n\n# YOUR CODE: print the sorted list"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Documentation: Text Inconsistencies\n\nAnswer these questions:\n\n**1. How many unique department values did you start with, and how many after cleaning?**\n\n*YOUR ANSWER*\n\n**2. Did text standardization create any new duplicates? How many?**\n\n*YOUR ANSWER*\n\n**3. What other text cleaning might still be needed? (Think about accents: Narino vs. Nari√±o)**\n\n*YOUR ANSWER*\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Part 6: Invalid Values\n\nThis is the trickiest issue because the values are not missing, not the wrong type, and\nnot duplicates. They **exist** and **look like numbers**, but they are **logically impossible**.\n\nThree columns in this dataset represent percentages that must be between 0 and 100:\n- `bajo_peso_nacer` (% of births with low weight) cannot be negative or over 100\n- `partos_cesarea` (% cesarean deliveries) cannot be negative or over 100\n- `partos_institucionales` (% institutional deliveries) cannot be negative or over 100\n\nA value of **-3.5%** for low birth weight is impossible (you cannot have negative percentages).\nA value of **115%** for cesarean deliveries is impossible (percentages cap at 100%).\n\nThese errors slip past all automated checks. Only **domain knowledge** (knowing what\nvalid health indicators look like) can catch them.\n\n**Strategy:** Replace invalid values with NaN, then fill with the column median (same\napproach we used for missing values).\n\n**Key methods:**\n- `df[cols].describe().loc[['min', 'max']]` -- check ranges quickly\n- `df[col] < 0` -- boolean mask for negative values\n- `df[col] > 100` -- boolean mask for values over 100\n- `df.loc[mask, col] = np.nan` -- replace matching values with NaN\n\n**Expected result:** Approximately 8 negative values and 6 values over 100."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Step 6.1: Check min/max of percentage columns\n\nUse `describe()` on the percentage columns and look at the `min` and `max` rows.\nAny `min` below 0 or `max` above 100 means there are invalid values."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "percentage_cols = [\n    'bajo_peso_nacer', 'partos_cesarea', 'partos_institucionales',\n]\n\n# YOUR CODE: show describe() for only the min and max rows\n# Hint: df[percentage_cols].describe().loc[['min', 'max']]"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Step 6.2: Count the invalid values\n\nCount exactly how many values are below 0 and above 100 in each column."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE: count negatives (values < 0) per column\n# Hint: df[percentage_cols].lt(0).sum()\nnegatives = # YOUR CODE\nprint(\"Negative values (< 0) per column:\")\nprint(negatives[negatives > 0])\nprint(f\"Total negatives: {negatives.sum()}\")\n\nprint()\n\n# YOUR CODE: count values > 100 per column\n# Hint: df[percentage_cols].gt(100).sum()\nover_100 = # YOUR CODE\nprint(\"Values over 100 per column:\")\nprint(over_100[over_100 > 0])\nprint(f\"Total over 100: {over_100.sum()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Step 6.3: Fix invalid values\n\nFor each percentage column:\n1. Create a boolean mask: `(df[col] < 0) | (df[col] > 100)`\n2. Replace those values with NaN: `df.loc[mask, col] = np.nan`\n3. Fill NaN with the column median\n\nLoop through `percentage_cols` and apply this fix."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "total_fixed = 0\n\nfor col in percentage_cols:\n    # YOUR CODE: create mask for invalid values (< 0 or > 100)\n    invalid_mask = # YOUR CODE\n    n_invalid = invalid_mask.sum()\n    \n    if n_invalid > 0:\n        # YOUR CODE: replace invalid values with NaN\n        \n        # YOUR CODE: fill NaN with median\n        median_val = df[col].median()\n        \n        print(f\"{col}: fixed {n_invalid} invalid values (replaced with median {median_val:.2f})\")\n        total_fixed += n_invalid\n\nprint(f\"\\nTotal invalid values fixed: {total_fixed}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Step 6.4: Verify the fix\n\nConfirm that all percentage columns now have values between 0 and 100."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE: show min and max for percentage_cols after fix\n# Verify: no min < 0 and no max > 100\nprint(\"After fix -- Min and Max:\")\n# YOUR CODE\n\n\n# Count remaining invalid values\nremaining_invalid = (df[percentage_cols].lt(0).sum().sum() + df[percentage_cols].gt(100).sum().sum())\nprint(f\"\\nInvalid values remaining: {remaining_invalid}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Documentation: Invalid Values\n\nAnswer these questions:\n\n**1. How many negative values and how many values over 100 did you find?**\n\n*YOUR ANSWER*\n\n**2. Why is domain knowledge necessary to catch these errors? Could an automated tool find them?**\n\n*YOUR ANSWER*\n\n**3. An alternative approach is \"clipping\": set negatives to 0 and values >100 to 100. When would clipping be better than replacing with the median? When would it be worse?**\n\n*YOUR ANSWER*\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Part 7: Final Verification\n\nCompare the original dataset with your cleaned version to see the full impact of your work.\nThis cell is pre-filled. Run it and review the output."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 55)\nprint(\"  BEFORE CLEANING (original)\")\nprint(\"=\" * 55)\nprint(f\"  Rows:                {len(df_original)}\")\nprint(f\"  Total NaN:           {df_original.isnull().sum().sum()}\")\nprint(f\"  Duplicates:          {df_original.duplicated().sum()}\")\nprint(f\"  Unique departments:  {df_original['departamento'].nunique()}\")\nprint(f\"  ano dtype:           {df_original['ano'].dtype}\")\nprint(f\"  cod_municipio dtype: {df_original['cod_municipio'].dtype}\")\n\nprint()\n\nprint(\"=\" * 55)\nprint(\"  AFTER CLEANING\")\nprint(\"=\" * 55)\nprint(f\"  Rows:                {len(df)}\")\nprint(f\"  Total NaN:           {df.isnull().sum().sum()}\")\nprint(f\"  Duplicates:          {df.duplicated().sum()}\")\nprint(f\"  Unique departments:  {df['departamento'].nunique()}\")\nprint(f\"  ano dtype:           {df['ano'].dtype}\")\nprint(f\"  cod_municipio dtype: {df['cod_municipio'].dtype}\")\n\nprint()\n\nprint(\"=\" * 55)\nprint(\"  PERCENTAGE COLUMNS RANGE CHECK\")\nprint(\"=\" * 55)\npct_check = ['bajo_peso_nacer', 'partos_cesarea', 'partos_institucionales']\nfor col in pct_check:\n    print(f\"  {col}: min={df[col].min():.2f}, max={df[col].max():.2f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Verification Checklist\n\nBefore submitting, verify each item. Change `[ ]` to `[x]` for each one you confirm:\n\n- [ ] **Missing values:** Zero NaN remaining (or justified remaining)\n- [ ] **Data types:** `ano` is `int64`, `cod_municipio` is `int64`\n- [ ] **Duplicates:** Zero duplicate rows remaining\n- [ ] **Text:** ~22 unique departments (all uppercase, no extra spaces)\n- [ ] **Invalid values:** All percentage columns between 0 and 100\n- [ ] **Row count:** ~492 rows (original 532 minus duplicates and dropped rows)\n- [ ] **All documentation cells filled in** with your reasoning\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Part 8: Reflection\n\nAnswer each question thoughtfully. These are graded.\n\n### 1. What was the most challenging data quality issue to fix? Why?\n\n*YOUR ANSWER*\n\n### 2. Which cleaning decisions required domain expertise about health data?\n\n*YOUR ANSWER*\n\n### 3. Data cleaning is iterative: fixing one problem can create another. Where did you experience this in the workshop?\n\n*YOUR ANSWER*\n\n### 4. How will you apply these steps to your project dataset from datos.gov.co?\n\n*YOUR ANSWER*\n\n### 5. If you had to clean this dataset again from scratch, what would you do differently?\n\n*YOUR ANSWER*\n\n### 6. Connection to your capstone project\n\n| Workshop Skill | Project Application |\n|----------------|---------------------|\n| Missing value handling | Clean your datos.gov.co dataset |\n| Type conversion | Ensure numeric columns are numeric |\n| Duplicate removal | Remove any duplicated records |\n| Text standardization | Standardize categorical values for grouping |\n| Domain validation | Identify impossible values using domain knowledge |\n| Documentation | Data cleaning section of your report |\n\n---\n\n*Week 3 Workshop -- Data Analytics Course -- Universidad Cooperativa de Colombia*"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}