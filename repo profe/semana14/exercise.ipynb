{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Week 14: In-Class Exercise - Your First Machine Learning Model\n",
    "\n",
    "## Objective\n",
    "Build a Decision Tree model end-to-end using the Water Consumption dataset.\n",
    "\n",
    "## Time: ~30 minutes\n",
    "\n",
    "## Dataset\n",
    "Water Consumption data from datos.gov.co.\n",
    "\n",
    "### What You Will Do:\n",
    "1. Load and prepare the data (features and target)\n",
    "2. Split into training and test sets\n",
    "3. Train a Decision Tree\n",
    "4. Evaluate the model and check for overfitting\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Run this cell to load the necessary libraries and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, r2_score,\n",
    "    accuracy_score\n",
    ")\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Water Consumption dataset from datos.gov.co\n",
    "url = \"https://www.datos.gov.co/resource/k9gy-47jj.csv?$limit=10000\"\n",
    "\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "print(f\"Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick data inspection\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and missing values\n",
    "print(\"Data Types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\nNumeric columns summary:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Data Preparation (10 minutes)\n",
    "\n",
    "Before building a model, we need to:\n",
    "1. Identify numeric columns\n",
    "2. Choose a target variable\n",
    "3. Select features\n",
    "4. Handle missing values\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify potential target and feature columns\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(\"Numeric columns:\")\n",
    "for col in numeric_cols:\n",
    "    print(f\"  - {col}: min={df[col].min():.2f}, max={df[col].max():.2f}, mean={df[col].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select target variable (consumption-related column)\n",
    "consumption_candidates = [col for col in numeric_cols if any(x in col.lower() for x in ['consumo', 'consumption', 'valor', 'cantidad', 'total'])]\n",
    "print(f\"Potential target columns: {consumption_candidates}\")\n",
    "\n",
    "# Select the target column (update based on your dataset)\n",
    "if consumption_candidates:\n",
    "    target_col = consumption_candidates[0]\n",
    "else:\n",
    "    target_col = df[numeric_cols].var().idxmax()\n",
    "\n",
    "print(f\"\\nSelected target column: {target_col}\")\n",
    "print(f\"Target statistics:\")\n",
    "print(df[target_col].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features (X) and target (y)\n",
    "# Use all numeric columns except the target\n",
    "feature_cols = [col for col in numeric_cols if col != target_col]\n",
    "\n",
    "# Remove columns that are IDs or codes\n",
    "id_patterns = ['id', 'codigo', 'code', 'key']\n",
    "feature_cols = [col for col in feature_cols if not any(p in col.lower() for p in id_patterns)]\n",
    "\n",
    "print(f\"Feature columns: {feature_cols}\")\n",
    "print(f\"Number of features: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and prepare data\n",
    "df_clean = df[feature_cols + [target_col]].dropna()\n",
    "\n",
    "X = df_clean[feature_cols]\n",
    "y = df_clean[target_col]\n",
    "\n",
    "print(f\"Samples after cleaning: {len(X)}\")\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Train/Test Split (5 minutes)\n",
    "\n",
    "Split the data so we can evaluate honestly.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "### Task 2.1: Split the data into 80% training and 20% test\n",
    "\n",
    "Use `train_test_split` with `test_size=0.2` and `random_state=42`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.1: Split data\n",
    "# YOUR CODE HERE\n",
    "X_train, X_test, y_train, y_test = ___\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Train a Decision Tree (10 minutes)\n",
    "\n",
    "Remember the sklearn pattern:\n",
    "1. Create the model\n",
    "2. Fit on training data\n",
    "3. Predict on test data\n",
    "4. Evaluate\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "### Task 3.1: Create and train a Decision Tree Regressor\n",
    "\n",
    "Use `max_depth=5` to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.1: Create and train the model\n",
    "\n",
    "# Step 1: Create the model\n",
    "# YOUR CODE HERE\n",
    "model = ___\n",
    "\n",
    "# Step 2: Fit the model on training data\n",
    "# YOUR CODE HERE\n",
    "___\n",
    "\n",
    "print(\"Decision Tree model trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "### Task 3.2: Make predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.2: Make predictions\n",
    "# YOUR CODE HERE\n",
    "y_pred = ___\n",
    "\n",
    "print(f\"First 5 predictions: {y_pred[:5]}\")\n",
    "print(f\"First 5 actual values: {y_test.values[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Evaluate and Check for Overfitting (5 minutes)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"=== DECISION TREE RESULTS ===\")\n",
    "print(f\"RMSE (Root Mean Squared Error): {rmse:.4f}\")\n",
    "print(f\"MAE (Mean Absolute Error): {mae:.4f}\")\n",
    "print(f\"R-squared: {r2:.4f}\")\n",
    "print(f\"\\nInterpretation: The model explains {r2*100:.1f}% of the variance in the target.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for overfitting: Compare train vs test performance\n",
    "y_pred_train = model.predict(X_train)\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "\n",
    "print(f\"Training R-squared: {r2_train:.4f}\")\n",
    "print(f\"Test R-squared:     {r2:.4f}\")\n",
    "print(f\"Gap:                {r2_train - r2:.4f}\")\n",
    "\n",
    "if r2_train - r2 > 0.1:\n",
    "    print(\"\\nWarning: Possible overfitting! Consider reducing max_depth.\")\n",
    "else:\n",
    "    print(\"\\nGood: No significant overfitting detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: Actual vs Predicted\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "ax.scatter(y_test, y_pred, alpha=0.5, color='steelblue')\n",
    "ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "\n",
    "ax.set_xlabel('Actual Values', fontsize=12)\n",
    "ax.set_ylabel('Predicted Values', fontsize=12)\n",
    "ax.set_title(f'Decision Tree: Actual vs Predicted\\nR-squared = {r2:.4f}', fontsize=14)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus: Try Different max_depth Values\n",
    "\n",
    "See how changing the tree depth affects overfitting.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus: Compare different max_depth values\n",
    "depths = [2, 3, 5, 10, 20, None]  # None = unlimited depth\n",
    "results = []\n",
    "\n",
    "for depth in depths:\n",
    "    dt = DecisionTreeRegressor(max_depth=depth, random_state=42)\n",
    "    dt.fit(X_train, y_train)\n",
    "    \n",
    "    r2_train_d = r2_score(y_train, dt.predict(X_train))\n",
    "    r2_test_d = r2_score(y_test, dt.predict(X_test))\n",
    "    \n",
    "    results.append({\n",
    "        'max_depth': str(depth) if depth else 'None',\n",
    "        'R2_train': r2_train_d,\n",
    "        'R2_test': r2_test_d,\n",
    "        'gap': r2_train_d - r2_test_d\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"=== DEPTH COMPARISON ===\")\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"\\nNotice: As depth increases, training score goes up but the gap (overfitting) also increases!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this exercise, you learned:\n",
    "\n",
    "1. **The sklearn pattern**: Create -> Fit -> Predict -> Evaluate\n",
    "2. **Train/Test Split**: Always evaluate on data the model has not seen\n",
    "3. **Decision Tree Regressor**: Predicts continuous values using decision rules\n",
    "4. **Key Metrics**: RMSE, MAE, R-squared\n",
    "5. **Overfitting Detection**: Compare train vs test scores; limit max_depth to prevent it\n",
    "\n",
    "---\n",
    "\n",
    "*End of Exercise*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
